{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b415ceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab in the specific helper libraries we would like\n",
    "\n",
    "#notes, non-filtering by ID, push to csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "\n",
    "INPUT_DIR = os.getcwd() + \"\\\\..\\\\data\\\\input\"\n",
    "OUTPUT_DIR = os.getcwd() + \"\\\\..\\\\data\\\\output\"\n",
    "\n",
    "TEST_FILE_NAME = INPUT_DIR + \"\\\\Interactions 01_26_2022 after 10_31_2021.xlsx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d900d6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\dev\\Gerontechnology\\data wrangling\\src\\..\\data\\input\\Interactions 01_26_2022 after 10_31_2021.xlsx\n"
     ]
    }
   ],
   "source": [
    "#make sure we \n",
    "print(TEST_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a1bddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(TEST_FILE_NAME)\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27fcffb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1645610040\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d_str = '1/4/2022 9:54'\n",
    "def conv_t_hard(d_str):\n",
    "    #print(d_str)\n",
    "    #date,other = d_str.split('  ')\n",
    "    date,curr_time = d_str.split(' ')\n",
    "    #curr_time, ampm = other.split(' ')\n",
    "    month,day,year = date.split('/')\n",
    "    month = int(month)\n",
    "    year = int(year)\n",
    "    day = int(day)\n",
    "    hour,minute = curr_time.split(':')\n",
    "    hour = int(hour)\n",
    "    minute = int(minute)\n",
    "    j = 0\n",
    "    month_secs = 0\n",
    "    while j <= month:\n",
    "        if(j == 2):\n",
    "            month_secs += 2419200\n",
    "        elif(j == 4 or j == 6 or j == 9 or j == 11):\n",
    "            month_secs += 2592000\n",
    "        else:\n",
    "            month_secs += 2678400\n",
    "        j += 1\n",
    "    secs = (year-1970)*31536000 + month_secs + day*24*60*60 + hour*60*60 + minute*60\n",
    "    return(secs)\n",
    "out = conv_t_hard(d_str)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a23cc296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236460\n"
     ]
    }
   ],
   "source": [
    "def conv_time(t_string, day):\n",
    "    #print(t_string)\n",
    "    time,dmark = t_string.split(' ')\n",
    "    hour,minutes = time.split(':')\n",
    "    offset = 0\n",
    "    if(dmark == 'PM'):\n",
    "        offset += 12*60*60\n",
    "    secs = 60*60*int(hour) + 60*int(minutes) + offset +int(day)*24*60*60\n",
    "    return(secs)\n",
    "sec = conv_time('5:41 PM', 2)\n",
    "print(sec)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d44b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_by_day(day, sec_gap, parc_time, parc_occur,df):\n",
    "    '''\n",
    "    we want two dicts: one that holds the last occurance of each participant,\n",
    "    and one that holds the number of times a gap of size n secs has occured\n",
    "    '''\n",
    "    ids = df['participantId_'+str(day)].dropna()\n",
    "    times = df['Time_'+str(day)].dropna()\n",
    "    i = 0\n",
    "    for rows in ids:\n",
    "        curr_id = ids[i]\n",
    "        curr_time = times[i]\n",
    "\n",
    "        if curr_id in parc_time.keys():\n",
    "            old_time = parc_time.get(curr_id)\n",
    "            new_time = times[i]\n",
    "            new_secs = conv_time(new_time, day)\n",
    "            if(new_secs - old_time >= sec_gap):\n",
    "                if curr_id in parc_occur:\n",
    "                    curr_count = parc_occur.get(curr_id)\n",
    "                    parc_occur.update({curr_id:curr_count+1})\n",
    "                else:\n",
    "                    parc_occur.update({curr_id:1})\n",
    "            parc_time.update({curr_id:new_secs})\n",
    "        else:\n",
    "            new_time = times[i]\n",
    "            new_secs = conv_time(new_time, day)\n",
    "            parc_time.update({curr_id:new_secs})\n",
    "\n",
    "        i += 1\n",
    "    return(parc_time, parc_occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02b37fe8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'participantId_20'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Reaga\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3629\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3628\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3630\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Reaga\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Reaga\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'participantId_20'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m parc_occur \u001b[39m=\u001b[39m {}\n\u001b[0;32m     10\u001b[0m \u001b[39mwhile\u001b[39;00m day \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m end_day:\n\u001b[1;32m---> 11\u001b[0m     parc_time, parc_occur \u001b[39m=\u001b[39m cal_by_day(day, sec_gap, parc_time, parc_occur, df)\n\u001b[0;32m     12\u001b[0m     \u001b[39mif\u001b[39;00m(\u001b[39mlen\u001b[39m(parc_ids) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m     13\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m----------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn [15], line 6\u001b[0m, in \u001b[0;36mcal_by_day\u001b[1;34m(day, sec_gap, parc_time, parc_occur, df)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcal_by_day\u001b[39m(day, sec_gap, parc_time, parc_occur,df):\n\u001b[0;32m      2\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m    we want two dicts: one that holds the last occurance of each participant,\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m    and one that holds the number of times a gap of size n secs has occured\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     ids \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mparticipantId_\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39mstr\u001b[39;49m(day)]\u001b[39m.\u001b[39mdropna()\n\u001b[0;32m      7\u001b[0m     times \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mTime_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(day)]\u001b[39m.\u001b[39mdropna()\n\u001b[0;32m      8\u001b[0m     i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Reaga\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Reaga\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3631\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3630\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3631\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3632\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3633\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3634\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3635\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3636\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'participantId_20'"
     ]
    }
   ],
   "source": [
    "day = 20\n",
    "sec_gap = 200\n",
    "end_day = 23\n",
    "parc_ids = [] #the ids we actually care about, leave empty if we want all\n",
    "event_ids = []\n",
    "\n",
    "parc_time = {}\n",
    "parc_occur = {}\n",
    "\n",
    "while day <= end_day:\n",
    "    parc_time, parc_occur = cal_by_day(day, sec_gap, parc_time, parc_occur, df)\n",
    "    if(len(parc_ids) == 0):\n",
    "        print(\"----------------------------\")\n",
    "        print(\"----------------------------\")\n",
    "        print(\"For Day:\" + str(day))\n",
    "        print(\"Times are:\")\n",
    "        print(parc_time)\n",
    "\n",
    "        print(\"Occurrences are:\")\n",
    "        print(parc_occur)\n",
    "\n",
    "        print(\"----------------------------\")\n",
    "        print(\"----------------------------\")\n",
    "        \n",
    "    day += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdfa0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique occurences:\n",
      "95\n",
      "Number of unique occurences:\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "for p in parc_ids:\n",
    "    print(\"Number of unique occurences:\")\n",
    "    occ = parc_occur.get(p)\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e2936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = pd.read_excel(TEST_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f781c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_by_day(d_str,curr_day):\n",
    "    month,day,dontcare = d_str.split('/')\n",
    "    if(int(day)!= curr_day):\n",
    "        return(1)\n",
    "    return(0)\n",
    "def get_current_day(d_str):\n",
    "    #print(d_str)\n",
    "    month,day,dontcare = d_str.split('/')\n",
    "    return(int(day))\n",
    "\n",
    "def cal_by_day_single(day, sec_gap, parc_time, parc_occur,df,el_ids,curr_day):\n",
    "    '''\n",
    "    we want two dicts: one that holds the last occurance of each participant,\n",
    "    and one that holds the number of times a gap of size n secs has occured\n",
    "    '''\n",
    "    ids = df['participantId'].dropna()\n",
    "    times = df['timestamp_local'].dropna()\n",
    "    i = 0\n",
    "    for rows in ids:\n",
    "        curr_id = ids[i]\n",
    "        curr_time = times[i]\n",
    "\n",
    "        if curr_id in parc_time.keys():\n",
    "            old_time = parc_time.get(curr_id)\n",
    "            new_time = times[i]\n",
    "            new_secs = conv_t_hard(new_time)\n",
    "            if(new_secs - old_time >= sec_gap):\n",
    "                if curr_id in parc_occur:\n",
    "                    if(len(el_ids) == 0 or df['elementId'][i] in el_ids):\n",
    "                        curr_count = parc_occur.get(curr_id)\n",
    "                        parc_occur.update({curr_id:curr_count+1})\n",
    "                else:\n",
    "                    parc_occur.update({curr_id:1})\n",
    "            parc_time.update({curr_id:new_secs})\n",
    "        else:\n",
    "            new_time = times[i]\n",
    "            new_secs = conv_t_hard(new_time)\n",
    "            parc_time.update({curr_id:new_secs})\n",
    "\n",
    "        i += 1\n",
    "    return(parc_time, parc_occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43f2ddd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_long' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [13], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m         i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     20\u001b[0m     \u001b[39mreturn\u001b[39;00m(new_df\u001b[39m.\u001b[39mT)\n\u001b[1;32m---> 22\u001b[0m new_df \u001b[39m=\u001b[39m filter_df_to_correct_days(df_long, \u001b[39m3\u001b[39m,\u001b[39m4\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[39m#print(df_long.shape)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mprint\u001b[39m(new_df\u001b[39m.\u001b[39mhead(\u001b[39m3\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_long' is not defined"
     ]
    }
   ],
   "source": [
    "def filter_df_to_correct_days(df, first_day, end_day):\n",
    "    #print(df.loc[0, 'timestamp_local'])\n",
    "    \n",
    "    new_df = pd.DataFrame()\n",
    "    #new_df.columns = df.columns\n",
    "    f_day = df.loc[0, 'timestamp_local']\n",
    "    current_date = get_current_day(f_day)\n",
    "    curr_d_index = 1\n",
    "    i = 0\n",
    "    while i < df.shape[0]:\n",
    "        new_date = get_current_day(df.loc[i, 'timestamp_local'])\n",
    "        if(new_date != current_date):\n",
    "            curr_d_index += 1\n",
    "            current_date = new_date \n",
    "        #print(curr_d_index, first_day)\n",
    "        if(int(curr_d_index) >= first_day and int(curr_d_index) <= end_day):\n",
    "            #new_df.append(df.loc[i])\n",
    "            new_df = pd.concat([new_df, df.loc[i]],axis=1)\n",
    "        i += 1\n",
    "    return(new_df.T)\n",
    "    \n",
    "new_df = filter_df_to_correct_days(df_long, 3,4)\n",
    "#print(df_long.shape)\n",
    "print(new_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c817e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put access calculations here:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0190fb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Documents/GitHub/hydraGAN_code_time_series/all_long.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_long \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39m./Documents/GitHub/hydraGAN_code_time_series/all_long.csv\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m#Put the path to the csv you care about here\u001b[39;00m\n\u001b[0;32m      2\u001b[0m start_day \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m# day to start\u001b[39;00m\n\u001b[0;32m      3\u001b[0m sec_gap \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m \u001b[39m#how many seconds between interactions before we care about it\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Reaga\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Reaga\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    663\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    664\u001b[0m     dialect,\n\u001b[0;32m    665\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    674\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    675\u001b[0m )\n\u001b[0;32m    676\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 678\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Reaga\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Reaga\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:932\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    929\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    931\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\Reaga\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1216\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1212\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1213\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1216\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1217\u001b[0m     f,\n\u001b[0;32m   1218\u001b[0m     mode,\n\u001b[0;32m   1219\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1220\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1221\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1222\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1223\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1224\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1225\u001b[0m )\n\u001b[0;32m   1226\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Reaga\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    782\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    785\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 786\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    787\u001b[0m             handle,\n\u001b[0;32m    788\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    789\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    790\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    791\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    792\u001b[0m         )\n\u001b[0;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Documents/GitHub/hydraGAN_code_time_series/all_long.csv'"
     ]
    }
   ],
   "source": [
    "df_long = pd.read_csv('./Documents/GitHub/hydraGAN_code_time_series/all_long.csv') #Put the path to the csv you care about here\n",
    "start_day = 1 # day to start\n",
    "sec_gap = 20 #how many seconds between interactions before we care about it\n",
    "end_day = 2 #day to end\n",
    "parc_ids = [] #the ids we actually care about, leave empty if we want all\n",
    "elem_ids = [] #the elements we want, leave blank if we want all\n",
    "\n",
    "parc_time = {}\n",
    "parc_occur = {}\n",
    "\n",
    "'''\n",
    "filter by day starting day\n",
    "\n",
    "dict to csv (locally)\n",
    "'''\n",
    "#curr_day = get_current_day(df_long['timestamp_local'][0])\n",
    "\n",
    "#print(\"We are starting on day:\", curr_day)\n",
    "df_long = filter_df_to_correct_days(df_long,start_day,end_day)\n",
    "\n",
    "\n",
    "parc_time, parc_occur = cal_by_day_single(day, sec_gap, parc_time, parc_occur, df_long,elem_ids, curr_day)\n",
    "if(len(parc_ids) == 0):\n",
    "    print(\"----------------------------\")\n",
    "    print(\"----------------------------\")\n",
    "    #print(\"For Day:\" + str(day))\n",
    "    print(\"Times are:\")\n",
    "    #print(parc_time)\n",
    "\n",
    "    print(\"Occurrences are:\")\n",
    "    print(parc_occur)\n",
    "\n",
    "    print(\"----------------------------\")\n",
    "    print(\"----------------------------\")\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8998a14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 14\n",
      "75 1\n"
     ]
    }
   ],
   "source": [
    "with open('./output.csv', 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile) \n",
    "    csvwriter.writerow(['Participant', 'Count'])\n",
    "    for key,value in parc_occur.items():\n",
    "        print(key,value)\n",
    "        csvwriter.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0145303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_sunday(original_datetime):\n",
    "    \"\"\"Returns the date of the next Sunday relative to the given date.\n",
    "\n",
    "    Args:\n",
    "        original_datetime (Datetime): A date\n",
    "\n",
    "    Returns:\n",
    "        Datetime: This will be a Sunday datetime.\n",
    "    \"\"\"\n",
    "    days_left = 7 - (original_datetime.floor(freq='D').day_of_week + 1) # offset needed because day of week starts on monday\n",
    "    if(days_left == 0):\n",
    "        days_left = 7\n",
    "    return (pd.to_datetime(original_datetime) + timedelta(days=days_left)).floor(freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dce1810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_to_day_of_week(df, day=0):\n",
    "    \"\"\"Filters and creates a new DataFrame of interactions that only includes entries of the given day of the week. \n",
    "    Precondition: DataFrame only includes a week's worth set of data.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): At most a week's worth of interactions\n",
    "        day (int, optional): Discriminates further on query to only when its this day of the week (Sunday = 0, Saturday = 6), Defaults to 0.\n",
    "    \"\"\"\n",
    "     # Filter df to only include interactions on this day of the week\n",
    "    end_date = next_sunday(df['timestamp_local'].max()) - timedelta(days=(6-day))\n",
    "    start_date = end_date - timedelta(days=1)\n",
    "    new_df = df.loc[(df[\"timestamp_local\"] >= start_date) & (df[\"timestamp_local\"] < end_date)]\n",
    "    return new_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7e07bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_to_distinct_interactions(df):\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19e07ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interaction_counts(df, elementIDs, day_of_week=-1, distinct=False):\n",
    "    \"\"\"Given a DataFrame which contains participant interactions,\n",
    "    returns a count of interactions for given elementIDs for each participant.\n",
    "\n",
    "    Args:\n",
    "        df (Pandas DataFrame): Created earlier from read_excel\n",
    "        elementIDs (list): A list of elementIDs where each elementID number is to be aggregated.\n",
    "        day_of_week (int, optional): Discriminates further on query to only when its this day of the week (Sunday = 0, Saturday = 6)\n",
    "        distinct (bool, optional): Allows us to filter the dataset further to only count distinct uses (5 minute intervals). Defaults to False.\n",
    "    \"\"\"\n",
    "    query_string = \"\"\n",
    "\n",
    "    # Filter the Dataset further to only include rows from the given day of the week.\n",
    "    if(day_of_week >= 0):\n",
    "        df = filter_to_day_of_week(df, day_of_week)\n",
    "    \n",
    "    # Filter the Dataset further to only include rows that are distinct uses.\n",
    "    if(distinct):\n",
    "        df = filter_to_distinct_interactions(df)    \n",
    "\n",
    "    # create a SQL string to filter DataFrame to only include rows with the desired interaction elementIDs\n",
    "    for i in range(len(elementIDs)):\n",
    "        query_string += 'elementId == {}'.format(elementIDs[i])\n",
    "        if((i+1) < len(elementIDs)):\n",
    "            query_string += \" or \"\n",
    "    \n",
    "    count = df.query(query_string) # count is a new DataFrame that only includes row entries with the given elementIDs\n",
    "    grouping1 = count.groupby(['participantId', 'elementId']).size() # groups each elementID to how many times each participant used it.\n",
    "    \n",
    "    return grouping1.groupby(['participantId']).sum() # returns the sum of each elementIDs use for each participant\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "305b65e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_variable(participants_dict, interactions_df, variable_name, elementIDs, variable_func, day_of_week=-1, distinct=False):\n",
    "    \"\"\"Creates a new variable and calculates it, storing the results for each participant in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        participants_dict (Dict): Participant data, key = participantID, value = dict of variables\n",
    "        interactions_df (DataFrame): An already created DataFrame that is data collected from some excel file\n",
    "        variable_name (string): Identifier by which this variable is known as.\n",
    "        elementIDs (list): A list of elementIDs that are counted and summed for this variable.\n",
    "        variable_func (Lambda Function): Describes how the aggregated count will be calculated, typically division due to weekly or daily averages.\n",
    "        day_of_week (int, optional): Allows us to filter the dataset further to only look at interactions on a specific day of the week (0 = Sunday to 6 = Saturday). Defaults to -1.\n",
    "        distinct (bool, optional): Allows us to filter the dataset further to only count distinct uses (5 minute intervals). Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "         Dict: An updated dictionary containing the calculated variable for all participants in the dataset, interactions_df\n",
    "    \"\"\"\n",
    "    \n",
    "    # Each interaction is a pair (participantID, count)\n",
    "    element_count_list = get_interaction_counts(interactions_df, elementIDs, day_of_week=day_of_week, distinct=distinct).iteritems()\n",
    "\n",
    "    for element_count in element_count_list:\n",
    "        participant_id = element_count[0]\n",
    "\n",
    "        if participant_id in participants_dict:\n",
    "            participants_dict[participant_id][variable_name] = variable_func(element_count[1])\n",
    "        else:\n",
    "            participants_dict[participant_id] = dict()\n",
    "            participants_dict[participant_id][variable_name] = variable_func(element_count[1])\n",
    "    \n",
    "    return participants_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ad8f2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def participant_dict_to_csv(participants, outfile_name=\"output.csv\"):\n",
    "    \"\"\"Given an instantiated dictionary, converts to a DataFrame to then be translated to a csv file.\n",
    "\n",
    "    Args:\n",
    "        participants (Dict): Participant data, key = participantID, value = dict of variables\n",
    "        outfile_name (str, optional): The name of the file to be write to. Relative path, using global OUTPUT_DIR. Defaults to \"output.csv\".\n",
    "    \"\"\"\n",
    "    interactions_df = pd.DataFrame.from_dict(participants, orient='index')\n",
    "    interactions_df.reset_index(inplace=True)\n",
    "    interactions_df.rename({'index':'participantId'}, axis='columns', inplace=True)\n",
    "    interactions_df.to_csv(OUTPUT_DIR + \"\\\\{}\".format(outfile_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec5a6596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weekly_interaction_dfs(interactions_filename):\n",
    "    \"\"\"Given a csv file, converts a dataset of interactions spanning many weeks into many datasets by week.\n",
    "\n",
    "    Args:\n",
    "        interactions_filename (string): Absolute filepath of excel file.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Dictionary of interaction DataFrames. Key = StartDate, Value=DataFrame\n",
    "    \"\"\"\n",
    "    weekly_dfs = dict()\n",
    "    interactions = pd.read_excel(interactions_filename)\n",
    "\n",
    "    start_date = interactions['timestamp_local'].min()\n",
    "    end_parsec = next_sunday(start_date)\n",
    "    end_date = interactions['timestamp_local'].max()\n",
    "\n",
    "    total = 0\n",
    "    while(start_date < end_date):\n",
    "        df = interactions.loc[(interactions[\"timestamp_local\"] >= start_date) & (interactions[\"timestamp_local\"] < end_parsec)]\n",
    "        weekly_dfs[(start_date.strftime(\"%U\"), start_date.strftime(\"%Y\"))] = df        \n",
    "        start_date = end_parsec\n",
    "        end_parsec = next_sunday(start_date)\n",
    "    return weekly_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d19a51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variable_calculations(df, outfile_name):\n",
    "    \"\"\"Given a dataset of interactions from participants, computes and stores calculation dependent variables to an output file.\n",
    "\n",
    "    Args:\n",
    "        interactions_filename (string): Excel file where all interaction data is stored (absolute path)\n",
    "        data_date_range (int, optional): The range of days by which this data was collected. Defaults to 7 (week's worth of data).\n",
    "    \"\"\"\n",
    "    participants = dict()\n",
    "\n",
    "    variableNames = [\"CalenderUse\", \"SumTotalCalendarInteractions\", \"CalendaringGoal\", \"TodayPageUse-Sunday\", \"TodayPageUse-Monday\", \"TodayPageUse-Tuesday\",\n",
    "                     \"TodayPageUse-Wednesday\", \"TodayPageUse-Thursday\", \"TodayPageUse-Friday\", \"TodayPageUse-Saturday\", \"SumTotalEventInteractions\", \"TodayPageGoal-Sunday\",\n",
    "                    \"TodayPageGoal-Monday\", \"TodayPageGoal-Tuesday\", \"TodayPageGoal-Wednesday\", \"TodayPageGoal-Thursday\", \"TodayPageGoal-Friday\", \"TodayPageGoal-Saturday\",\n",
    "                    \"LTGFolderUse\", \"SumTotalLTGNoteInteractions\", \"LTGGoal\", \"FZFolderUse\", \"SumTotalFZNoteInteractions\", \"FZGoal\"]\n",
    "\n",
    "    participants = create_variable(participants, df, \"CalenderUse\", [9], lambda x: x/(7))\n",
    "    participants = create_variable(participants, df, \"SumTotalCalendarInteractions\", [9, 18, 19, 20], lambda x: x/(7))\n",
    "    participants = create_variable(participants, df, \"CalendaringGoal\", [9], lambda x: x/(4))\n",
    "\n",
    "    participants = create_variable(participants, df, \"TodayPageUse-Sunday\",    [8, 13, 379, 380, 381, 384], lambda x: x, day_of_week=0, distinct=True)\n",
    "    participants = create_variable(participants, df, \"TodayPageUse-Monday\",    [8, 13, 379, 380, 381, 384], lambda x: x, day_of_week=1, distinct=True)\n",
    "    participants = create_variable(participants, df, \"TodayPageUse-Tuesday\",   [8, 13, 379, 380, 381, 384], lambda x: x, day_of_week=2, distinct=True)\n",
    "    participants = create_variable(participants, df, \"TodayPageUse-Wednesday\", [8, 13, 379, 380, 381, 384], lambda x: x, day_of_week=3, distinct=True)\n",
    "    participants = create_variable(participants, df, \"TodayPageUse-Thursday\",  [8, 13, 379, 380, 381, 384], lambda x: x, day_of_week=4, distinct=True)\n",
    "    participants = create_variable(participants, df, \"TodayPageUse-Friday\",    [8, 13, 379, 380, 381, 384], lambda x: x, day_of_week=5, distinct=True)\n",
    "    participants = create_variable(participants, df, \"TodayPageUse-Saturday\",  [8, 13, 379, 380, 381, 384], lambda x: x, day_of_week=6, distinct=True)\n",
    "\n",
    "    participants = create_variable(participants, df, \"SumTotalEventInteractions\", [13, 103, 104, 379, 380, 381, 384], lambda x: x/(7))\n",
    "\n",
    "    participants = create_variable(participants, df, \"TodayPageGoal-Sunday\",    [13, 103, 104, 379, 380, 381, 384], lambda x: x/(3), day_of_week=0, distinct=True)\n",
    "    participants = create_variable(participants, df, \"TodayPageGoal-Monday\",    [13, 103, 104, 379, 380, 381, 384], lambda x: x/(3), day_of_week=1, distinct=True)\n",
    "    participants = create_variable(participants, df, \"TodayPageGoal-Tuesday\",   [13, 103, 104, 379, 380, 381, 384], lambda x: x/(3), day_of_week=2, distinct=True)\n",
    "    participants = create_variable(participants, df, \"TodayPageGoal-Wednesday\", [13, 103, 104, 379, 380, 381, 384], lambda x: x/(3), day_of_week=3, distinct=True)\n",
    "    participants = create_variable(participants, df, \"TodayPageGoal-Thursday\",  [13, 103, 104, 379, 380, 381, 384], lambda x: x/(3), day_of_week=4, distinct=True)\n",
    "    participants = create_variable(participants, df, \"TodayPageGoal-Friday\",    [13, 103, 104, 379, 380, 381, 384], lambda x: x/(3), day_of_week=5, distinct=True)\n",
    "    participants = create_variable(participants, df, \"TodayPageGoal-Saturday\",  [13, 103, 104, 379, 380, 381, 384], lambda x: x/(3), day_of_week=6, distinct=True)\n",
    "\n",
    "    # ! Resolve function zone token v. LTG token\n",
    "    participants = create_variable(participants, df, \"LTGFolderUse\",  [24, 220, 322], lambda x: x/(7), distinct=True)\n",
    "    participants = create_variable(participants, df, \"SumTotalLTGNoteInteractions\",  [24, 64, 65, 220], lambda x: x/(7))\n",
    "    participants = create_variable(participants, df, \"LTGGoal\",  [24, 220, 322], lambda x: x)\n",
    "    participants = create_variable(participants, df, \"FZFolderUse\",  [24, 220, 322], lambda x: x/(7), distinct=True)\n",
    "    participants = create_variable(participants, df, \"SumTotalFZNoteInteractions\",  [24, 64, 65, 220], lambda x: x/(7))\n",
    "    participants = create_variable(participants, df, \"FZGoal\",  [24, 220, 322], lambda x: x) # ! this is currently the same as LTGGoal\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # set the values of variables not calculated to 0 (the use of them by he participants never appeared in the dataset)\n",
    "    for participant_id in participants:\n",
    "        keys = participants[participant_id].keys()\n",
    "        undeclared_variables = list(set(variableNames).difference(keys)) # these are the variables for each participant that were not calculated. (=0)\n",
    "        for variable in undeclared_variables:\n",
    "            participants[participant_id][variable] = 0\n",
    "    participant_dict_to_csv(participants, outfile_name=outfile_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "400cc6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weekly_calculations_table(interactions_filename):\n",
    "    \"\"\"Given a large dataset of interactions from multiple participants, outputs all weekly csv files from that data showcasing desired calculated variables.\n",
    "\n",
    "    Args:\n",
    "        interactions_filename (string): Absolute path to large dataset\n",
    "    \"\"\"\n",
    "    weekly_dfs = get_weekly_interaction_dfs(interactions_filename)\n",
    "\n",
    "    for date, df in weekly_dfs.items():\n",
    "        filename = \"Week {}, {}\".format(date[0], date[1])\n",
    "        print(filename)\n",
    "        get_variable_calculations(df, \"{}.csv\".format(filename))\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91ffd316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week 41, 2021\n",
      "Week 42, 2021\n",
      "Week 43, 2021\n",
      "Week 44, 2021\n",
      "Week 45, 2021\n",
      "Week 46, 2021\n",
      "Week 47, 2021\n",
      "Week 48, 2021\n",
      "Week 49, 2021\n",
      "Week 50, 2021\n",
      "Week 51, 2021\n",
      "Week 52, 2021\n",
      "Week 01, 2022\n",
      "Week 02, 2022\n",
      "Week 03, 2022\n",
      "Week 04, 2022\n"
     ]
    }
   ],
   "source": [
    "# given test file, all interaction variable calculations will go to output directory.\n",
    "create_weekly_calculations_table(TEST_FILE_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e103691567e4ddce5972f9ef292c473f85cc6b67cbb8b1f014bd2975fa0d78f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
